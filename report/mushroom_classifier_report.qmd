---
title: Mushroom Edibility Classification Using Feature-Based Machine Learning Approach
author:
  - name: "Benjamin Frizzell"
    affiliation: "UBC Master of Data Science"
  - name: "Hankun Xiao"
    affiliation: "UBC Master of Data Science"
  - name: "Essie Zhang"
    affiliation: "UBC Master of Data Science"
  - name: "Mason Zhang"
    affiliation: "UBC Master of Data Science"

date: "2024-12-07"
jupyter: python3
format: 
    html:
        toc: true
        toc-depth: 2
    pdf:
        toc: true
        toc-depth: 2
bibliography: references.bib
execute:
  echo: false
  warning: false
editor: source
---

```{python}
import pandas as pd
from IPython.display import Markdown
test_confusion_matrix = pd.read_csv("../results/tables/test_confusion_matrix.csv", index_col=0)
# Extract values from the confusion matrix
TP = test_confusion_matrix.loc['p', 'p']  # True Positives: Predicted 'p' and Actual 'p'
FP = test_confusion_matrix.loc['e', 'p']  # False Positives: Predicted 'p' but Actual 'e'
TN = test_confusion_matrix.loc['e', 'e']  # True Negatives: Predicted 'e' and Actual 'e'
FN = test_confusion_matrix.loc['p', 'e']  # False Negatives: Predicted 'e' but Actual 'p'
CORRECT = TP + TN
TOTAL = TP + FP + TN + FN
ACCURACY = round(CORRECT/TOTAL, 3)
PRECISION = TP / (TP + FP)
RECALL = TP / (TP + FN)
BETA = 2

percent_poison = round(100*(TP + FN)/TOTAL,2)
percent_edible = round(100*(TN + FP)/TOTAL,2)

def f_beta(precision, recall, beta):
    f_beta_score = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)
    return round(f_beta_score, 3)

F_BETA = f_beta(PRECISION, RECALL, BETA)
```

## Summary
In this project, we developed and evaluated a machine learning model to classify mushroom edibility (i.e., whether they are edible or poisonous) based on features such as color, habitat, and structure. This is an important task as eating poisonous mushrooms can lead to severe health risks, while identifying toxic mushrooms through manual visual inspection can be challenging and time-consuming.

We built and fine-tuned a Support Vector Classifier (SVC), using both accuracy and $F_{\beta}$ as performance metrics. The $F_{\beta}$ score is a weighted harmonic mean of precision and recall. We calculated $F_{\beta}$ with $\beta = 2$ to place greater emphasis on recall, as predicting a poisonous mushroom as edible (false negatives) could have severe consequences. Our final model achieved an overall accuracy of `{python} ACCURACY` and $F_{\beta}$ score with $\beta = `{python} BETA`$ of `{python} F_BETA` with `{python} CORRECT` correct predictions out of `{python} TOTAL` test observations. In terms of error, the model  predicted `{python} FN` poisonous mushrooms as edible (false negative) and predicted `{python} FP` edible mushrooms as poisonous (false positive). 

Even though the model shows promising performance, a limitation of this project is the lack of incorporation of biological domain knowledge about mushrooms into the feature engineering process, which could further reduce false negatives. This is an area that could be addressed in future work.

## Introduction

Mushrooms are a widely consumed food source known for their rich nutritional content, including essential vitamins and minerals. However, not all mushrooms are safe for consumption, as many species are highly toxic. Eating poisonous mushrooms can lead to severe health consequences, including hospitalization and death, as highlighted in an incident report from @cdc2024.

Distinguishing between edible and poisonous mushrooms is particularly challenging, especially when dealing with large quantities. According to research by @niosh2019, in the commercial distribution of agricultural products, there is a risk of mixing edible and poisonous mushrooms. Traditional identification methods rely on human expertise, which can be time-consuming and inconsistent.

The primary goal of this project is to develop an accurate machine learning model that minimizes false negatives (i.e., misclassifying poisonous mushrooms as edible) while maintaining reasonable overall performance. To achieve this, our project applies machine learning techniques to classify mushroom edibility based on various features, such as color, habitat, and structure. The dataset used in this study contains detailed observations of mushrooms, categorized as either edible or poisonous @secondary_mushroom_848. Our analysis applied three classification algorithms: Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), and Logistic Regression. These models were chosen for their diverse approaches to classification and their ability to capture complex relationships between features.

## Methods

### Data

The dataset used in this project is the Secondary Mushroom Dataset @secondary_mushroom_848. This dataset contains 61069 hypothetical mushrooms with caps based on 173 species (353 mushrooms per species). Each mushroom is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended (the latter class was combined with the poisonous class).

### Analysis

The mushroom dataset is balanced with `{python} percent_poison`% of poisonous mushroom and `{python} percent_edible`% of edible mushroom. All variables were standardized and variables with more than 15% missing values are dropped, because imputing a variable that has a significant proportion of missing data might introduce too much noise or bias, making it unreliable. Data was splitted with 80% being partitioned into the training set and 20% being partitioned into the test set. Three classification models including Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), and Logistic Regression are used to predict whether a mushroom is edible or poisonous. The fine tuned Support Vector Classifier has the best overall performance. The hyperparameter was chosen using 5-fold cross validation with $F_{\beta}$ score as the classification metric. $\beta$ was chosen to be set to `{python} BETA` for the $F_{\beta}$ score to increase the weight on recall during fitting because predicting a mushroom to be edible when it is in fact poisonous could have severe health consequences. Therefore the goal is to prioritize the minimization of false negatives. The Python programming language @Python and the following Python packages were used to perform the analysis: Matplotlib @4160265), Pandas: @mckinney-proc-scipy-2010, Scikit-learn: @scikit-learn, NumPy: @harris2020array, SciPy: @2020SciPy-NMeth, UCIMLRepo: @secondary_mushroom_848., Pandera: @niels_bantilan-proc-scipy-2020, Pytest: @pytest8.3.4, and Deepchecks: @Chorev_Deepchecks_A_Library_2022.

## Exploratory data analysis

### Part 1: Numeric Features
![The Distribution of Feature Cap Diameter](../results/figures/cap-diameter_histogram.png){#fig-cap-diameter width=40%}

![The Distribution of Feature Stem Height](../results/figures/stem-height_histogram.png){#fig-stem-height width=40%}

![The Distribution of Feature Stem Width](../results/figures/stem-width_histogram.png){#fig-stem-width width=40%}

\newpage
The EDA shows that all numeric columns in the mushroom dataset are nearly normal with some skewness. A robust preprocessing scheme `QuantileTransformer` is used because it can transform skewed data or heavy-tailed distributions into a more Gaussian-like shape and reduce the impact of outliers. `OneHotEncoder` is applied for categorical features in the mushroom dataset, because each feature does not contains much categories and they are not ordered. It is critical to keep all important information in the features. Since ring type feature has many missing values, it was filled in with a "Missing" class. Treating missing values as a distinct category provides a way to model the absence of data directly. This can be valuable because missingness itself might carry information; for example, some mushrooms may not have particularly unique ring types that cannot be classified into the other groups, or simply may not have rings at all.

### Part 2: Categorical Features

```{python}
#| label: tbl-cap-shape
#| tbl-cap: The distribution of cap shape.

table_to_display = pd.read_csv("../results/tables/cap-shape_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-cap-color
#| tbl-cap: The distribution of mushroom cap colors.

table_to_display = pd.read_csv("../results/tables/cap-color_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-does-bruise-or-bleed
#| tbl-cap: The distribution of mushrooms that bruise or bleed.

table_to_display = pd.read_csv("../results/tables/does-bruise-or-bleed_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-gill-color
#| tbl-cap: The distribution of mushroom gill colors.

table_to_display = pd.read_csv("../results/tables/gill-color_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-stem-color
#| tbl-cap: The distribution of mushroom stem colors.

table_to_display = pd.read_csv("../results/tables/stem-color_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-has-ring
#| tbl-cap: The distribution of samples that have and do not have rings.

table_to_display = pd.read_csv("../results/tables/has-ring_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-ring-type
#| tbl-cap: The distribution of mushroom ring types.

table_to_display = pd.read_csv("../results/tables/ring-type_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-habitat
#| tbl-cap: The distribution of mushroom habitats.

table_to_display = pd.read_csv("../results/tables/habitat_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```

```{python}
#| label: tbl-season
#| tbl-cap: The distribution of the seasons of which mushrooms are present.

table_to_display = pd.read_csv("../results/tables/season_summary.csv")
Markdown(table_to_display.to_markdown(index = False))
```
```{python}
#| label: tbl-columns-dropped
#| tbl-cap: Features dropped from the original dataset prior to processing.

table_to_display = pd.read_csv("../data/processed/columns_to_drop.csv")
Markdown(table_to_display.to_markdown(index = False))
```


\newpage
Based on the Frequency and Percentage distributions, here are our findings:

1.  @tbl-cap-shape: The most common cap shape is `x` (convex), comprising 43.97% of the data. Other shapes like `f` (flat) and `s` (sunken) are also prevalent, while `c` (conical) is the least common with 2.95% appearance.

2.  @tbl-cap-color: The most frequently appeared color is `n` (brown), with 39.71% of the data. Other colors like `y` (yellow), `w` (white), and `g` (gray) are also well-represented, while rare colors like `b` (buff) and `l` (blue) appear in less than 2% of the data.

3.  @tbl-does-bruise-or-bleed: The majority of the mushrooms are `f` (do not bruise or bleed), while their counterpart makes up 17.26% of the data.

4.  @tbl-gill-color: The most common gill color is `w` (white), with 30.45% of the data. Other colors such as `n` (brown) and `y` (yellow) are also frequent, while rare gill colors like `e` (red), `b` (buff), and `u` (purple) appear in less than 2% of the data.

5.  @tbl-stem-color: `w` (white) and `n` (brown) are the dominating stem colors, accounting for 37.75% and 29.5% of the data, respectively. Other colors like `r` (green), `l` (blue), and `b` (buff) are less frequent, appearing in less than 1% of the observations.

6.  @tbl-has-ring: Most mushrooms are `f` (do not have a ring), with 74.84% observations. The remaining 25.16% mushrooms are `t` (have a ring).

7.  @tbl-ring-type: `f` (none) is the most common ring type, accounting for 82.3% of the data. Other types like `e` (evanescent) and `z` (zone) are less frequent, while rare types like `m` (movable) occur in less than 1% of the data.

8.  @tbl-habitat: The predominant habitat is `d` (woods), with 72.46% appearance. Other habitats such as `g` (grasses) and `l` (leaves) are less common, while `w` (waste), `p` (paths), and `u` (urban) only make up less than 1% of the data individually.

9.   @tbl-season: Most mushrooms grow in `a` (autumn), comprising 49.36% of the data, followed by `u` (summer) at 37.5%. The other two seasons `w` (winter) and `s` (spring) are less frequent.

Categorical features will be encoded into binary format in the following preprocessing phase with `OneHotEncoder`. Since we are dealing with a mix of binary and non-binary categorical features, for features like `does-bruise-or-bleed` and `has-ring` that have two unique values, they will be handled with `drop='if_binary'` argument to reduce redundancy while still capturing the information.

It should be noted that many categorical features from the original dataset were dropped: See @tbl-columns-dropped for a full list. These features were found to have a substantial, and in some cases majority of entries missing. In this case, it appeared unreasonable to make assumptions about how to fill such entries without stronger domain-specific knowledge, so these features were dropped.

### Part 4: The distribution of the target

![The Distribution of Feature Stem Color](../results/figures/target_variable_distribution.png){#fig-target width=50%}

The target variable `class` represents whether a mushroom is `p` (poisonous) or `e` (edible). Understanding the distribution of the target helps assessing class balance, which might have impact on models' performance.

Based on the Frequency and Percentage distribution @fig-target, here are our findings:

1.  `p` (Poisonous): There are 27,143 instances of poisonous mushrooms, accounting for 55.56% of the data.

2.  `e` (Edible): There are 21,712 instances of edible mushrooms, constituting 44.44% of the data.

Using $F_{\beta}$, precision, recall, or confusion matrix to evaluate the model's performance is advisable in the following procedure.

\newpage
## Result & Discussion
### Preprocessing and Model Building
```{python}
import pandas as pd
cv_result = pd.read_csv("../results/tables/cross_val_results.csv")
cv_result.rename(columns={'Unnamed: 0': 'Model'}, inplace=True)
cv_result = cv_result[['Model', 'mean_test_accuracy', 'mean_test_f2_score']]

cv_knn_accuracy = round(cv_result.loc[0, 'mean_test_accuracy'], 3)
cv_knn_f2_score = round(cv_result.loc[0, 'mean_test_f2_score'], 3)

cv_logreg_accuracy = round(cv_result.loc[1, 'mean_test_accuracy'], 3)
cv_logreg_f2_score = round(cv_result.loc[1, 'mean_test_f2_score'], 3)

cv_svc_accuracy = round(cv_result.loc[2, 'mean_test_accuracy'], 3)
cv_svc_f2_score = round(cv_result.loc[2, 'mean_test_f2_score'], 3)
```

Three classification models including Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), and Logistic Regression are used to predict whether a mushroom is edible or poisonous. Predicting a mushroom to be edible when it is in fact poisonous could have severe health consequences. Therefore the best model should prioritize the minimization of this error. To do this, we can evaluate models on an $F_{\beta}$ score with $\beta = `{python} BETA`$.


```{python}
#| label: tbl-cv
#| tbl-cap: Cross-Validation Results by Model 
Markdown(cv_result.to_markdown(index = False))
```

In @tbl-cv, we can see that after tuning the hyperparameter, the Logistic Regression model has the mean accuracy of `{python} cv_logreg_accuracy` and mean $F_{\beta}$ score of `{python} cv_logreg_f2_score` on the validation set. The KNN model has the mean accuracy of `{python} cv_knn_accuracy` and mean $F_{\beta}$ score of `{python} cv_knn_f2_score`. The SVC outperforms both Logistic Regression and KNN significantly in both accuracy of `{python} cv_svc_accuracy` and $F_{\beta}$ score of `{python} cv_svc_f2_score`. Thus, SVC is the ideal choice to identify edible or poisonous mushroom (recall is the highest priority), however it should be noted that this model takes extensive time to train. If faster training is preferred, the Logistic Regression model may be chosen as a suitable alternative due to comparable performance and a much faster fitting time. 

\newpage
### Model Evaluation

```{python}
#| label: tbl-test-confusion-matrix
#| tbl-cap: Confusion matrix for the SVM model on test data.

table_to_display = pd.read_csv("../results/tables/test_confusion_matrix.csv")
Markdown(table_to_display.to_markdown(index = False))
```

The prediction model performed quite well on test data, with a final overall accuracy of `{python} ACCURACY` and $F_{\beta}$ score of `{python} F_BETA` @tbl-test-confusion-matrix. The model only makes `{python} FN + FP` mistakes out of `{python} TOTAL` test samples. `{python} FN` mistakes were predicting a poisonous mushroom as edible (false negative), while `{python} FP` mistakes were predicting a edible mushroom as poisonous (false positive). The model’s performance is promising for implementation, as false negatives represent potential safety risks and these errors could lead to consuming poisonous mushrooms, it is minimized to protect users. On the other hand, false positives are less harmful, they may lead to discarding safe mushrooms unnecessarily but do not endanger safety.

While the overall performance of the SVC model are impressive, efforts could focus on further reducing false negatives to enhance the safety of predictions. It might be important to take a closer look at the `{python} FN + FP` misclassified observations to identify specific features contributing to these misclassifications. Implementing feature engineering on those features such as encoding rare categories differently can enhance the model’s power and reduce the misclassification cases. Additionally, trying other classifiers like Decision Tree and Random Forest which are less sensitive to scaling or irrelevant features might improve the prediction.

### Limitation and Future Work

While the model yield promising performance, a key limitation is the lack of feature engineering based on domain-specific biological knowledge. There may be interactions between features or specific biological metrics with strong predictive power that were not considered. In future work, integrating such domain knowledge could enhance model performance and further reduce false negatives.

Additionally, the model still makes mistakes, including false positives and false negatives. Further investigation into the misclassified cases could help us identify patterns and feature interactions that the model failed to capture. These insights could make improvements in both feature engineering and model architecture.

\newpage
## References
